# Kafka-Spark-MongoDB-Simulation

This project demonstrates a data pipeline simulation using Apache Kafka, Apache Spark, and MongoDB. It integrates these technologies to simulate data ingestion, real-time processing, and storage in a NoSQL database. This setup is a typical example of a modern data processing architecture used for big data applications.

## Table of Contents

- [Introduction](#introduction)
- [Architecture](#architecture)
- [Features](#features)
- [Contributors](#contributors)

## Introduction

This project showcases a simulation of a real-time data processing pipeline. The pipeline leverages:

- **Apache Kafka** for data ingestion and messaging.
- **Apache Spark** for real-time data processing and analytics.
- **MongoDB** for storage of processed data.

The project can be used as a reference or a starting point for building scalable and robust data pipelines.

## Architecture

The architecture of the project is as follows:

1. **Kafka**: Acts as the message broker, handling incoming data streams.
2. **Spark**: Consumes data from Kafka, processes it in real-time, and performs necessary transformations and computations.
3. **MongoDB**: Stores the processed data for further querying and analysis.

## Features

- Real-time data ingestion using Kafka.
- Stream processing using Spark.
- Storage of processed data in MongoDB.
- Example data simulation to demonstrate the pipeline.

## Contributors

- [Stamelos Charilaos Panagiotis](https://github.com/stamelosxp) 
- [Karesiou Andreas](https://github.com/karesioua22)